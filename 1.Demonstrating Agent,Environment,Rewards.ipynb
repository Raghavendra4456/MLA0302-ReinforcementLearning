{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrfnwo+3aw+/E9asdPGml2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghavendra4456/MLA0302-ReinforcementLearning/blob/main/1.Demonstrating%20Agent%2CEnvironment%2CRewards.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbaF65ACJ1_i",
        "outputId": "17e95156-0c5c-40bb-be68-c9f2b6378ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 10.0\n",
            "Episode 10, Total Reward: 9.0\n",
            "Episode 20, Total Reward: 9.0\n",
            "Episode 30, Total Reward: 10.0\n",
            "Episode 40, Total Reward: 9.0\n",
            "Episode 50, Total Reward: 9.0\n",
            "Episode 60, Total Reward: 9.0\n",
            "Episode 70, Total Reward: 9.0\n",
            "Episode 80, Total Reward: 9.0\n",
            "Episode 90, Total Reward: 9.0\n",
            "Episode 100, Total Reward: 10.0\n",
            "Episode 110, Total Reward: 8.0\n",
            "Episode 120, Total Reward: 10.0\n",
            "Episode 130, Total Reward: 9.0\n",
            "Episode 140, Total Reward: 10.0\n",
            "Episode 150, Total Reward: 10.0\n",
            "Episode 160, Total Reward: 9.0\n",
            "Episode 170, Total Reward: 10.0\n",
            "Episode 180, Total Reward: 9.0\n",
            "Episode 190, Total Reward: 9.0\n",
            "Episode 200, Total Reward: 10.0\n",
            "Episode 210, Total Reward: 10.0\n",
            "Episode 220, Total Reward: 9.0\n",
            "Episode 230, Total Reward: 9.0\n",
            "Episode 240, Total Reward: 9.0\n",
            "Episode 250, Total Reward: 9.0\n",
            "Episode 260, Total Reward: 9.0\n",
            "Episode 270, Total Reward: 9.0\n",
            "Episode 280, Total Reward: 8.0\n",
            "Episode 290, Total Reward: 8.0\n",
            "Episode 300, Total Reward: 11.0\n",
            "Episode 310, Total Reward: 10.0\n",
            "Episode 320, Total Reward: 10.0\n",
            "Episode 330, Total Reward: 10.0\n",
            "Episode 340, Total Reward: 8.0\n",
            "Episode 350, Total Reward: 9.0\n",
            "Episode 360, Total Reward: 10.0\n",
            "Episode 370, Total Reward: 9.0\n",
            "Episode 380, Total Reward: 9.0\n",
            "Episode 390, Total Reward: 9.0\n",
            "Episode 400, Total Reward: 9.0\n",
            "Episode 410, Total Reward: 8.0\n",
            "Episode 420, Total Reward: 9.0\n",
            "Episode 430, Total Reward: 9.0\n",
            "Episode 440, Total Reward: 8.0\n",
            "Episode 450, Total Reward: 9.0\n",
            "Episode 460, Total Reward: 9.0\n",
            "Episode 470, Total Reward: 10.0\n",
            "Episode 480, Total Reward: 8.0\n",
            "Episode 490, Total Reward: 10.0\n",
            "Episode 500, Total Reward: 10.0\n",
            "Episode 510, Total Reward: 9.0\n",
            "Episode 520, Total Reward: 9.0\n",
            "Episode 530, Total Reward: 10.0\n",
            "Episode 540, Total Reward: 10.0\n",
            "Episode 550, Total Reward: 9.0\n",
            "Episode 560, Total Reward: 9.0\n",
            "Episode 570, Total Reward: 10.0\n",
            "Episode 580, Total Reward: 9.0\n",
            "Episode 590, Total Reward: 9.0\n",
            "Episode 600, Total Reward: 11.0\n",
            "Episode 610, Total Reward: 9.0\n",
            "Episode 620, Total Reward: 8.0\n",
            "Episode 630, Total Reward: 8.0\n",
            "Episode 640, Total Reward: 10.0\n",
            "Episode 650, Total Reward: 10.0\n",
            "Episode 660, Total Reward: 10.0\n",
            "Episode 670, Total Reward: 10.0\n",
            "Episode 680, Total Reward: 9.0\n",
            "Episode 690, Total Reward: 10.0\n",
            "Episode 700, Total Reward: 11.0\n",
            "Episode 710, Total Reward: 10.0\n",
            "Episode 720, Total Reward: 9.0\n",
            "Episode 730, Total Reward: 10.0\n",
            "Episode 740, Total Reward: 10.0\n",
            "Episode 750, Total Reward: 10.0\n",
            "Episode 760, Total Reward: 9.0\n",
            "Episode 770, Total Reward: 10.0\n",
            "Episode 780, Total Reward: 9.0\n",
            "Episode 790, Total Reward: 9.0\n",
            "Episode 800, Total Reward: 9.0\n",
            "Episode 810, Total Reward: 9.0\n",
            "Episode 820, Total Reward: 9.0\n",
            "Episode 830, Total Reward: 9.0\n",
            "Episode 840, Total Reward: 9.0\n",
            "Episode 850, Total Reward: 9.0\n",
            "Episode 860, Total Reward: 9.0\n",
            "Episode 870, Total Reward: 9.0\n",
            "Episode 880, Total Reward: 9.0\n",
            "Episode 890, Total Reward: 9.0\n",
            "Episode 900, Total Reward: 9.0\n",
            "Episode 910, Total Reward: 8.0\n",
            "Episode 920, Total Reward: 9.0\n",
            "Episode 930, Total Reward: 8.0\n",
            "Episode 940, Total Reward: 9.0\n",
            "Episode 950, Total Reward: 9.0\n",
            "Episode 960, Total Reward: 10.0\n",
            "Episode 970, Total Reward: 9.0\n",
            "Episode 980, Total Reward: 11.0\n",
            "Episode 990, Total Reward: 9.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Neural network for the agent\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Agent class\n",
        "class Agent:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.policy_network = PolicyNetwork(input_size, output_size)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.01)\n",
        "        self.output_size = output_size  # Store output size\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(state).float()\n",
        "        probabilities = self.policy_network(state)\n",
        "        probabilities = probabilities.detach().numpy()  # Convert to numpy array\n",
        "        action = np.random.choice(np.arange(self.output_size), p=probabilities)\n",
        "        return action\n",
        "\n",
        "# Training loop\n",
        "agent = Agent(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n",
        "num_episodes = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        agent.optimizer.zero_grad()\n",
        "        state_tensor = torch.from_numpy(state).float()\n",
        "        action_tensor = torch.tensor(action)\n",
        "        reward_tensor = torch.tensor(reward)\n",
        "\n",
        "        log_prob = torch.log(agent.policy_network(state_tensor)[action_tensor])\n",
        "        loss = -log_prob * reward_tensor\n",
        "        loss.backward()\n",
        "        agent.optimizer.step()\n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}, Total Reward: {episode_reward}\")\n",
        "\n",
        "env.close()\n"
      ]
    }
  ]
}